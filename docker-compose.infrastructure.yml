# =============================================================================
# STAGE 2: NLP PROCESSING SERVICE - INFRASTRUCTURE INTEGRATION
# =============================================================================
# This file connects Stage 2 to the centralized Storytelling Platform
# infrastructure. It MUST be used for production/staging deployments.
# =============================================================================

networks:
  storytelling:
    external: true
    name: storytelling

services:
  # ===========================================================================
  # NER SERVICE (Named Entity Recognition) - CPU-only
  # ===========================================================================
  ner-service:
    container_name: nlp-ner-service
    build:
      context: .
      dockerfile: Dockerfile_ner
    image: stage2-nlp-ner:latest
    networks:
      - storytelling
    environment:
      - SERVICE_NAME=stage2-nlp-ner
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Force CPU-only mode to free GPU for Event LLM
      - CUDA_VISIBLE_DEVICES=
      - TRANSFORMERS_CACHE=/app/models/cache
    volumes:
      - huggingface_cache:/app/models/cache
      # Development mount for hot-reload
      - ./src:/app/src:ro
      - ./config:/app/config:ro
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
          # No GPU reservation - runs on CPU
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - "stage=2"
      - "service=ner"
      - "com.docker.compose.project=stage2-nlp"
    restart: unless-stopped

  # ===========================================================================
  # DP SERVICE (Dependency Parsing) - CPU-only
  # ===========================================================================
  dp-service:
    container_name: nlp-dp-service
    build:
      context: .
      dockerfile: Dockerfile_dp
    image: stage2-nlp-dp:latest
    networks:
      - storytelling
    environment:
      - SERVICE_NAME=stage2-nlp-dp
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Force CPU-only mode to free GPU for Event LLM
      - CUDA_VISIBLE_DEVICES=
      - TRANSFORMERS_CACHE=/app/models/cache
    volumes:
      - huggingface_cache:/app/models/cache
      # Development mount for hot-reload
      - ./src:/app/src:ro
      - ./config:/app/config:ro
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
          # No GPU reservation - runs on CPU
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - "stage=2"
      - "service=dp"
      - "com.docker.compose.project=stage2-nlp"
    restart: unless-stopped

  # ===========================================================================
  # EVENT LLM SERVICE (Event Extraction)
  # ===========================================================================
  event-llm-service:
    container_name: nlp-event-llm-service
    build:
      context: .
      dockerfile: Dockerfile_event_llm
    image: stage2-nlp-event-llm:latest
    networks:
      - storytelling
    # Required for vLLM inter-process communication
    shm_size: '16gb'
    environment:
      - SERVICE_NAME=stage2-nlp-event-llm
      - LOG_LEVEL=${LOG_LEVEL:-DEBUG}
      # Exclusive GPU access for vLLM (GPU 0)
      - CUDA_VISIBLE_DEVICES=0
      - TRANSFORMERS_CACHE=/app/models/cache
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      # vLLM-specific environment variables
      - VLLM_LOGGING_LEVEL=DEBUG
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
      - NCCL_P2P_DISABLE=1
      # Prevent PyTorch memory fragmentation
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - huggingface_cache:/app/models/cache
      # Development mount for hot-reload (source code)
      - ./src:/app/src:ro
      - ./config:/app/config:ro
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 24G
        reservations:
          cpus: '4'
          memory: 12G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    labels:
      - "stage=2"
      - "service=event-llm"
      - "com.docker.compose.project=stage2-nlp"
    restart: unless-stopped

  # ===========================================================================
  # ORCHESTRATOR SERVICE (FastAPI)
  # ===========================================================================
  orchestrator-service:
    container_name: nlp-orchestrator
    build:
      context: .
      dockerfile: Dockerfile_orchestrator
    image: stage2-nlp-orchestrator:latest
    networks:
      - storytelling
    # Expose port for development/testing (use Traefik for production)
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      # -------------------------------------------------------------------------
      # Service Configuration
      # -------------------------------------------------------------------------
      - SERVICE_NAME=stage2-nlp-orchestrator
      - ENVIRONMENT=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

      # -------------------------------------------------------------------------
      # Infrastructure Services (Redis)
      # -------------------------------------------------------------------------
      # Redis Broker (Celery) - Stage 2: (2-1)*2 = 2
      - REDIS_HOST=redis-broker
      - REDIS_PORT=6379
      - REDIS_DB=2

      # Redis Cache - Stage 2: (2-1)*2+1 = 3
      - REDIS_CACHE_HOST=redis-cache
      - REDIS_CACHE_PORT=6379
      - REDIS_CACHE_DB=3

      # Celery Configuration
      - CELERY_BROKER_URL=redis://redis-broker:6379/2
      - CELERY_RESULT_BACKEND=redis://redis-cache:6379/3

      # -------------------------------------------------------------------------
      # Infrastructure Services (PostgreSQL)
      # -------------------------------------------------------------------------
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=stage2_nlp
      - POSTGRES_USER=stage2_user
      - POSTGRES_PASSWORD=${STAGE2_POSTGRES_PASSWORD}

      # -------------------------------------------------------------------------
      # Infrastructure Services (Elasticsearch - Optional)
      # -------------------------------------------------------------------------
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_API_KEY=${ELASTICSEARCH_API_KEY:-}

      # -------------------------------------------------------------------------
      # Internal Service URLs
      # -------------------------------------------------------------------------
      - NER_SERVICE_URL=http://nlp-ner-service:8001
      - DP_SERVICE_URL=http://nlp-dp-service:8002
      - EVENT_LLM_SERVICE_URL=http://nlp-event-llm-service:8003

      # -------------------------------------------------------------------------
      # Metadata Registry (Shared Infrastructure)
      # -------------------------------------------------------------------------
      - METADATA_REGISTRY_ENABLED=${METADATA_REGISTRY_ENABLED:-true}
      - METADATA_PRIMARY_BACKEND=${METADATA_PRIMARY_BACKEND:-postgresql}
      - METADATA_ENABLE_REDIS_CACHE=${METADATA_ENABLE_REDIS_CACHE:-true}
      - METADATA_POSTGRES_HOST=postgres
      - METADATA_POSTGRES_PORT=5432
      - METADATA_POSTGRES_DB=pipeline_metadata
      - METADATA_POSTGRES_USER=metadata_service
      - METADATA_SERVICE_PASSWORD=${METADATA_SERVICE_PASSWORD}
      - METADATA_REDIS_HOST=redis-cache
      - METADATA_REDIS_PORT=6379
      - METADATA_REDIS_DB=15
      - METADATA_REDIS_TTL=86400

      # -------------------------------------------------------------------------
      # Observability
      # -------------------------------------------------------------------------
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317
      - OTEL_SERVICE_NAME=stage2-nlp-orchestrator

    volumes:
      - ./config:/app/config:ro
      - ./data:/app/data
      - ./logs:/app/logs
      - huggingface_cache:/root/.cache/huggingface
      # Development mount for hot-reload
      - ./src:/app/src:ro
      # Shared volumes for inter-stage communication
      - ../stage1-cleaning-service/shared_volume/stage1/output:/app/shared_volume/stage1/output:ro
      - ./shared_volume/stage2/output:/app/shared_volume/stage2/output:rw

    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    labels:
      - "stage=2"
      - "service=orchestrator"
      - "com.docker.compose.project=stage2-nlp"
      - "version=1.0.0"

    restart: unless-stopped

    depends_on:
      ner-service:
        condition: service_healthy
      dp-service:
        condition: service_healthy
      event-llm-service:
        condition: service_healthy

    # NO ports: directive - Traefik handles all routing
    # Access via: http://localhost/api/v1/nlp/*

  # ===========================================================================
  # CELERY WORKER (Batch Processing)
  # ===========================================================================
  celery-worker:
    container_name: nlp-celery-worker
    build:
      context: .
      dockerfile: Dockerfile_orchestrator
    image: stage2-nlp-worker:latest
    command: celery -A src.core.celery_tasks worker --loglevel=info --concurrency=4
    networks:
      - storytelling
    env_file:
      - .env
    environment:
      # -------------------------------------------------------------------------
      # Service Configuration
      # -------------------------------------------------------------------------
      - SERVICE_NAME=stage2-nlp-worker
      - ENVIRONMENT=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

      # -------------------------------------------------------------------------
      # Infrastructure Services (same as orchestrator)
      # -------------------------------------------------------------------------
      - REDIS_HOST=redis-broker
      - REDIS_PORT=6379
      - REDIS_DB=2
      - REDIS_CACHE_HOST=redis-cache
      - REDIS_CACHE_PORT=6379
      - REDIS_CACHE_DB=3

      - CELERY_BROKER_URL=redis://redis-broker:6379/2
      - CELERY_RESULT_BACKEND=redis://redis-cache:6379/3

      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=stage2_nlp
      - POSTGRES_USER=stage2_user
      - POSTGRES_PASSWORD=${STAGE2_POSTGRES_PASSWORD}

      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_API_KEY=${ELASTICSEARCH_API_KEY:-}

      # -------------------------------------------------------------------------
      # Internal Service URLs
      # -------------------------------------------------------------------------
      - NER_SERVICE_URL=http://nlp-ner-service:8001
      - DP_SERVICE_URL=http://nlp-dp-service:8002
      - EVENT_LLM_SERVICE_URL=http://nlp-event-llm-service:8003

      # -------------------------------------------------------------------------
      # Metadata Registry
      # -------------------------------------------------------------------------
      - METADATA_REGISTRY_ENABLED=${METADATA_REGISTRY_ENABLED:-true}
      - METADATA_PRIMARY_BACKEND=${METADATA_PRIMARY_BACKEND:-postgresql}
      - METADATA_ENABLE_REDIS_CACHE=${METADATA_ENABLE_REDIS_CACHE:-true}
      - METADATA_POSTGRES_HOST=postgres
      - METADATA_POSTGRES_PORT=5432
      - METADATA_POSTGRES_DB=pipeline_metadata
      - METADATA_POSTGRES_USER=metadata_service
      - METADATA_SERVICE_PASSWORD=${METADATA_SERVICE_PASSWORD}
      - METADATA_REDIS_HOST=redis-cache
      - METADATA_REDIS_PORT=6379
      - METADATA_REDIS_DB=15
      - METADATA_REDIS_TTL=86400

      # -------------------------------------------------------------------------
      # GPU Configuration (Optional)
      # -------------------------------------------------------------------------
      - CUDA_VISIBLE_DEVICES=${CELERY_GPU_ID:-}

      # -------------------------------------------------------------------------
      # Observability
      # -------------------------------------------------------------------------
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317
      - OTEL_SERVICE_NAME=stage2-nlp-worker

    volumes:
      - ./config:/app/config:ro
      - ./data:/app/data
      - ./logs:/app/logs
      - huggingface_cache:/root/.cache/huggingface
      # Shared volumes for inter-stage communication
      - ../stage1-cleaning-service/shared_volume/stage1/output:/app/shared_volume/stage1/output:ro
      - ./shared_volume/stage2/output:/app/shared_volume/stage2/output:rw

    deploy:
      resources:
        limits:
          cpus: '22'
          memory: 100G
        reservations:
          cpus: '12'
          memory: 50G

    healthcheck:
      test: ["CMD-SHELL", "celery -A src.core.celery_tasks inspect ping || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

    labels:
      - "stage=2"
      - "service=celery-worker"
      - "com.docker.compose.project=stage2-nlp"
      - "version=1.0.0"

    restart: unless-stopped

    depends_on:
      orchestrator-service:
        condition: service_healthy
      ner-service:
        condition: service_healthy
      dp-service:
        condition: service_healthy
      event-llm-service:
        condition: service_healthy

  # ===========================================================================
  # EVENT CONSUMER SERVICE (Stage 1 → Stage 2 Integration)
  # ===========================================================================
  # Consumes cleaned document events from Stage 1 and triggers NLP processing
  event-consumer:
    container_name: nlp-event-consumer
    build:
      context: .
      dockerfile: Dockerfile_orchestrator
    image: stage2-nlp-event-consumer:latest
    command: python3 -m src.services.event_consumer_service
    networks:
      - storytelling
    env_file:
      - .env
    environment:
      # -------------------------------------------------------------------------
      # Service Configuration
      # -------------------------------------------------------------------------
      - SERVICE_NAME=stage2-event-consumer
      - ENVIRONMENT=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

      # -------------------------------------------------------------------------
      # Event Consumer Configuration
      # -------------------------------------------------------------------------
      - EVENT_CONSUMER_ENABLED=${EVENT_CONSUMER_ENABLED:-true}
      - STAGE1_EVENT_STREAM=${STAGE1_EVENT_STREAM:-stage1:cleaning:events}
      - AUTO_PROCESS=${AUTO_PROCESS:-true}

      # -------------------------------------------------------------------------
      # Infrastructure Services (Redis - Stage 1 uses DB 1 for events)
      # -------------------------------------------------------------------------
      # Redis Cache DB 1 (where Stage 1 publishes events)
      - REDIS_HOST=redis-cache
      - REDIS_PORT=6379
      - REDIS_CACHE_DB=1

      # Redis Broker for Stage 2 (for triggering processing)
      - REDIS_BROKER_HOST=redis-broker
      - REDIS_BROKER_PORT=6379
      - REDIS_BROKER_DB=2

      # Celery Configuration (for triggering Stage 2 tasks)
      - CELERY_BROKER_URL=redis://redis-broker:6379/2
      - CELERY_RESULT_BACKEND=redis://redis-cache:6379/3

      # -------------------------------------------------------------------------
      # Infrastructure Services (PostgreSQL)
      # -------------------------------------------------------------------------
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=stage2_nlp
      - POSTGRES_USER=stage2_user
      - POSTGRES_PASSWORD=${STAGE2_POSTGRES_PASSWORD}

      # Stage 1 PostgreSQL (for reading cleaned documents)
      - STAGE1_POSTGRES_HOST=postgres
      - STAGE1_POSTGRES_PORT=5432
      - STAGE1_POSTGRES_DB=stage1_cleaning
      - STAGE1_POSTGRES_USER=stage1_user
      - STAGE1_POSTGRES_PASSWORD=${STAGE1_POSTGRES_PASSWORD:-}

      # -------------------------------------------------------------------------
      # Internal Service URLs (for triggering NLP processing)
      # -------------------------------------------------------------------------
      - ORCHESTRATOR_URL=http://nlp-orchestrator:8000
      - NER_SERVICE_URL=http://nlp-ner-service:8001
      - DP_SERVICE_URL=http://nlp-dp-service:8002
      - EVENT_LLM_SERVICE_URL=http://nlp-event-llm-service:8003

      # -------------------------------------------------------------------------
      # Shared Data Directory (Stage 1 → Stage 2 file transfer)
      # -------------------------------------------------------------------------
      - SHARED_STAGE1_DIR=/app/shared_volume/stage1/output

      # -------------------------------------------------------------------------
      # Observability
      # -------------------------------------------------------------------------
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317
      - OTEL_SERVICE_NAME=stage2-event-consumer

    volumes:
      - ./config:/app/config:ro
      - ./data:/app/data
      - ./logs:/app/logs
      # Shared volumes for inter-stage communication
      - ../stage1-cleaning-service/shared_volume/stage1/output:/app/shared_volume/stage1/output:ro

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep 'event_consumer_service' | grep -v grep || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

    labels:
      - "stage=2"
      - "service=event-consumer"
      - "com.docker.compose.project=stage2-nlp"
      - "version=1.0.0"
      - "integration=stage1-to-stage2"

    restart: unless-stopped

    depends_on:
      orchestrator-service:
        condition: service_healthy

volumes:
  huggingface_cache:
    driver: local

# =============================================================================
# IMPORTANT NOTES
# =============================================================================
# 1. NO port mappings - Traefik handles all routing
# 2. Access via: http://localhost/api/v1/nlp/*
# 3. Must register route in ../infrastructure/traefik/dynamic.yml
# 4. Ensure STAGE2_POSTGRES_PASSWORD is set in .env
# 5. Ensure METADATA_SERVICE_PASSWORD is set in .env
# 6. Redis uses DB 2 (broker) and DB 3 (cache) per Stage 2 allocation
# 7. This file does NOT define redis, postgres, or elasticsearch
#    - All infrastructure services are in ../infrastructure
# =============================================================================
