# Stage 2: NLP Processing Service

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/Docker-20.10+-blue.svg)](https://www.docker.com/)
[![CUDA](https://img.shields.io/badge/CUDA-12.1-green.svg)](https://developer.nvidia.com/cuda-toolkit)

**Event & Entity Extraction Pipeline with Sophisticated Storyline Distinction**

A production-ready microservices system that extracts events, entities, and relationships from unstructured text using state-of-the-art NLP models. Part of an 8-stage sequential storytelling pipeline that transforms raw news articles into coherent, temporal narratives.

---

## ğŸ¯ **Key Capabilities**

### **Storyline Distinction**
The system can distinguish between nuanced storylines involving the same entities:
- **Trump + Israel/Gaza conflict** vs **Trump + Qatar economic partnerships**
- **Russia/Ukraine military actions** vs **Russia/Ukraine diplomatic negotiations**
- **US tariffs on China** vs **US tariffs on EU**

**How?** Multi-dimensional similarity analysis combining:
- **Semantic similarity** (40%): Event description embeddings
- **Entity overlap** (30%): Entity-role-context triplets (`entity|role|domain|context_hash`)
- **Temporal proximity** (20%): Exponential decay within 7-day window
- **Domain similarity** (10%): Domain-aware classification (12 domains)

### **Performance**
- **vLLM optimization**: 15-25x speedup over standard transformers
- **GPU acceleration**: All models (NER, DP, LLM, embeddings)
- **Parallel processing**: 22 Dask workers for batch jobs
- **Throughput**: 100-300 documents/hour at full capacity

---

## ğŸ“‹ **Table of Contents**

- [Architecture](#architecture)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Configuration](#configuration)
- [Development](#development)
- [Troubleshooting](#troubleshooting)

---

## ğŸ—ï¸ **Architecture**

### **Microservices**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ORCHESTRATOR SERVICE (Port 8000)              â”‚
â”‚         Main API, coordinates pipeline & batches             â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚           â”‚            â”‚             â”‚
   â–¼           â–¼            â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NER â”‚  â”‚ DP  â”‚   â”‚ Event LLM  â”‚  â”‚Event Linker  â”‚
â”‚8001 â”‚  â”‚8002 â”‚   â”‚   8003     â”‚  â”‚  (in-proc)   â”‚
â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
   â”‚        â”‚            â”‚                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STORAGE BACKENDS                            â”‚
â”‚       JSONL   â”‚   PostgreSQL   â”‚   Elasticsearch            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Components**

| Service | Port | Model | Purpose |
|---------|------|-------|---------|
| **Orchestrator** | 8000 | - | Main API, coordinates pipeline |
| **NER Service** | 8001 | Babelscape/wikineural-multilingual-ner | Entity extraction |
| **DP Service** | 8002 | spaCy en_core_web_trf | Dependency parsing, SOA triplets |
| **Event LLM** | 8003 | Mistral-7B-Instruct-v0.3 (AWQ) | Event extraction with vLLM |
| **Event Linker** | - | all-mpnet-base-v2 | Storyline distinction |
| **Celery Worker** | - | All models | Batch processing with Dask (22 workers) |
| **Redis** | 6379 | - | Celery broker & caching |

---

## ğŸš€ **Quick Start**

### **Prerequisites**
- Docker Engine 20.10+ with Compose v2
- NVIDIA GPU with 16GB+ VRAM (RTX A4000 or better)
- NVIDIA Container Toolkit
- 48+ CPU cores, 160GB+ RAM (recommended)
- HuggingFace account with access token

### **1. Clone & Setup**

```bash
# Clone repository
cd /home/mshittu/projects/nlp/stage2-nlp-processing

# Create environment file
cp .env.example .env

# Edit .env and add your HuggingFace token
nano .env
# Set: HUGGINGFACE_TOKEN=hf_YOUR_TOKEN_HERE
```

### **2. Build & Start Services**

```bash
# Build Docker images (first time only, ~15 minutes)
./run.sh build

# Start all services
./run.sh start

# Check service status
./run.sh status
```

### **3. Process Your First Document**

```bash
# Using CLI
python -m src.cli.main documents process "President Biden met with Israeli PM Netanyahu in Washington to discuss Gaza."

# Using API
curl -X POST http://localhost:8000/v1/documents \
  -H "Content-Type: application/json" \
  -d @data/sample_stage1_documents.jsonl | jq '.'
```

### **4. Process Batch**

```bash
# Submit batch job
python -m src.cli.main documents batch data/sample_stage1_documents.jsonl

# Check job status (replace JOB_ID)
python -m src.cli.main jobs status <JOB_ID>

# Get results
python -m src.cli.main jobs results <JOB_ID> --output results.json
```

---

## ğŸ“¥ **Installation**

### **Method 1: Docker (Recommended)**

```bash
# Ensure NVIDIA Container Toolkit is installed
nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.1.0-base nvidia-smi

# Build and start
./run.sh build
./run.sh start
```

### **Method 2: Local Development**

```bash
# Create virtual environment
python3.10 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_trf

# Set environment variables
export HUGGINGFACE_TOKEN=hf_YOUR_TOKEN_HERE
export REDIS_HOST=localhost

# Start services individually
python -m src.api.ner_service &
python -m src.api.dp_service &
python -m src.api.event_llm_service &
python -m src.api.orchestrator_service &
```

---

## ğŸ’» **Usage**

### **CLI Commands**

```bash
# Process single document
nlp documents process "Your text here" --output results.json

# Process batch file (JSONL format)
nlp documents batch /path/to/documents.jsonl

# Check job status
nlp jobs status <job_id>

# Get job results
nlp jobs results <job_id> --output results.json

# Health check
nlp admin health

# List services
nlp admin services
```

### **Python SDK**

```python
import httpx
import json

# Process single document
document = {
    "document_id": "doc_001",
    "cleaned_text": "President Biden announced new policy...",
    "cleaned_title": "Biden Policy Announcement",
    "cleaned_publication_date": "2024-11-15T14:30:00Z"
}

response = httpx.post(
    "http://localhost:8000/v1/documents",
    json={"document": document}
)

result = response.json()
print(f"Found {len(result['result']['events'])} events")
print(f"Found {len(result['result']['extracted_entities'])} entities")

# Process batch
documents = [doc1, doc2, doc3]
batch_response = httpx.post(
    "http://localhost:8000/v1/documents/batch",
    json={"documents": documents}
)

job_id = batch_response.json()["job_id"]
print(f"Batch job submitted: {job_id}")
```

### **REST API**

```bash
# Health check
curl http://localhost:8000/health | jq '.'

# Process document
curl -X POST http://localhost:8000/v1/documents \
  -H "Content-Type: application/json" \
  -d '{
    "document": {
      "document_id": "test_001",
      "cleaned_text": "Your text here"
    }
  }' | jq '.'

# Check job status
curl http://localhost:8000/v1/jobs/<job_id> | jq '.'
```

---

## ğŸ“š **API Documentation**

### **Orchestrator API (Port 8000)**

#### **POST /v1/documents**
Process single document.

**Request:**
```json
{
  "document": {
    "document_id": "doc_001",
    "cleaned_text": "Text content...",
    "cleaned_title": "Article Title",
    "cleaned_publication_date": "2024-11-15T14:30:00Z"
  }
}
```

**Response:**
```json
{
  "success": true,
  "document_id": "doc_001",
  "result": {
    "document_id": "doc_001",
    "processed_at": "2024-11-15T15:45:30Z",
    "extracted_entities": [...],
    "extracted_soa_triplets": [...],
    "events": [...]
  },
  "processing_time_ms": 8542.3
}
```

#### **POST /v1/documents/batch**
Submit batch processing job.

**Request:**
```json
{
  "documents": [
    {"document_id": "doc_001", "cleaned_text": "..."},
    {"document_id": "doc_002", "cleaned_text": "..."}
  ],
  "batch_id": "batch_20241115"
}
```

**Response:**
```json
{
  "success": true,
  "batch_id": "batch_20241115",
  "job_id": "abc-def-123",
  "document_count": 2,
  "message": "Batch job submitted successfully"
}
```

#### **GET /v1/jobs/{job_id}**
Get job status and results.

**Response:**
```json
{
  "job_id": "abc-def-123",
  "status": "SUCCESS",
  "progress": 100.0,
  "documents_processed": 100,
  "documents_total": 100,
  "result": {
    "success_count": 98,
    "error_count": 2,
    "storylines": [...]
  }
}
```

#### **GET /health**
System health check.

**Response:**
```json
{
  "status": "ok",
  "services": {
    "ner_service": {"status": "healthy", "url": "http://ner-service:8001"},
    "dp_service": {"status": "healthy", "url": "http://dp-service:8002"},
    "event_llm_service": {"status": "healthy", "url": "http://event-llm-service:8003"}
  }
}
```

### **See Also**
- [Swagger UI](http://localhost:8000/docs) - Interactive API documentation
- [ReDoc](http://localhost:8000/redoc) - Alternative API documentation

---

## âš™ï¸ **Configuration**

### **Main Configuration: `config/settings.yaml`**

```yaml
# GPU & Performance
general:
  gpu_enabled: true
  device: "cuda"

# Event LLM with vLLM
event_llm_service:
  use_vllm: true
  gpu_memory_utilization: 0.90
  quantization: "awq"

# Dask Parallel Processing
celery:
  dask_local_cluster_n_workers: 22  # Half of CPU cores
  dask_cluster_total_memory: "140GB"

# Storyline Distinction
event_linking:
  enforce_domain_boundaries: true
  storyline_clustering:
    enabled: true
    weights:
      semantic: 0.4
      entity: 0.3
      temporal: 0.2
      domain: 0.1

# Storage Backends
storage:
  enabled_backends:
    - "jsonl"
    # - "postgresql"
    # - "elasticsearch"
```

### **Environment Variables: `.env`**

```bash
# Required
HUGGINGFACE_TOKEN=hf_YOUR_TOKEN_HERE

# Optional (defaults work for Docker)
REDIS_HOST=redis
POSTGRES_PASSWORD=your_secure_password
NER_SERVICE_URL=http://ner-service:8001
DP_SERVICE_URL=http://dp-service:8002
EVENT_LLM_SERVICE_URL=http://event-llm-service:8003
```

---

## ğŸ› ï¸ **Development**

### **Project Structure**

```
stage2-nlp-processing/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                    # FastAPI services
â”‚   â”‚   â”œâ”€â”€ ner_service.py
â”‚   â”‚   â”œâ”€â”€ dp_service.py
â”‚   â”‚   â”œâ”€â”€ event_llm_service.py
â”‚   â”‚   â””â”€â”€ orchestrator_service.py
â”‚   â”œâ”€â”€ core/                   # Core NLP logic
â”‚   â”‚   â”œâ”€â”€ ner_logic.py
â”‚   â”‚   â”œâ”€â”€ dp_logic.py
â”‚   â”‚   â”œâ”€â”€ event_llm_logic.py
â”‚   â”‚   â”œâ”€â”€ llm_prompts.py
â”‚   â”‚   â”œâ”€â”€ event_linker.py
â”‚   â”‚   â””â”€â”€ celery_tasks.py
â”‚   â”œâ”€â”€ schemas/                # Pydantic models
â”‚   â”‚   â””â”€â”€ data_models.py
â”‚   â”œâ”€â”€ storage/                # Storage backends
â”‚   â”‚   â””â”€â”€ backends.py
â”‚   â”œâ”€â”€ utils/                  # Utilities
â”‚   â”‚   â”œâ”€â”€ config_manager.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ document_processor.py
â”‚   â””â”€â”€ cli/                    # CLI interface
â”‚       â””â”€â”€ main.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml
â”œâ”€â”€ data/                       # Data directory
â”œâ”€â”€ logs/                       # Logs directory
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile_*
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.sh
â””â”€â”€ README.md
```

### **Running Tests**

```bash
# Unit tests (when implemented)
pytest tests/

# Integration tests
./run.sh test

# Manual testing
python -m src.cli.main documents process "Test text"
```

### **Viewing Logs**

```bash
# All services
./run.sh logs

# Specific service
./run.sh logs orchestrator
./run.sh logs ner-service
./run.sh logs celery-worker

# Follow logs
./run.sh logs -f orchestrator
```

### **Rebuilding Services**

```bash
# Rebuild all
./run.sh rebuild

# Rebuild specific service
./run.sh rebuild event-llm-service

# Rebuild without cache
./run.sh rebuild-no-cache orchestrator
```

---

## ğŸ› **Troubleshooting**

### **Common Issues**

#### **GPU Not Detected**
```bash
# Verify NVIDIA driver
nvidia-smi

# Verify Docker GPU support
docker run --rm --gpus all nvidia/cuda:12.1.0-base nvidia-smi

# Check container GPU access
docker exec nlp-ner-service nvidia-smi
```

#### **Out of Memory (OOM)**
```yaml
# Reduce in config/settings.yaml:
event_llm_service:
  gpu_memory_utilization: 0.80  # Default: 0.90
  max_batch_size: 2              # Default: 4

celery:
  dask_local_cluster_n_workers: 16  # Default: 22
```

#### **Service Unhealthy**
```bash
# Check logs
./run.sh logs <service-name>

# Restart service
./run.sh restart

# Rebuild if needed
./run.sh rebuild <service-name>
docker ps --filter "name=nlp-" --format "table {{.Names}}\t{{.Status}}
```

#### **Model Download Fails**
```bash
# Verify HuggingFace token
echo $HUGGINGFACE_TOKEN

# Manual download (run inside container)
./run.sh shell orchestrator
python -c "from transformers import AutoModel; AutoModel.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3')"
```

### **Getting Help**

- **Logs**: `./run.sh logs` shows detailed error messages
- **Health Check**: `./run.sh status` shows service health
- **Configuration**: Check `config/settings.yaml` for all settings
- **Documentation**: See `CLAUDE.md` for detailed component documentation

---

## ğŸ“Š **Performance Benchmarks**

### **Single Document Processing**
- Latency: 8-25 seconds per document
- NER: 2-5s
- DP: 2-5s
- Event LLM: 4-15s
- Event Linking: 1-3s

### **Batch Processing** (100 documents)
- Throughput: 100-300 docs/hour with 22 workers
- Average: ~30-60 minutes for 100 documents
- Memory: ~6GB per concurrent document
- GPU utilization: 85-95%

### **Hardware Recommendations**
- **Minimum**: 8GB RAM, 4-core CPU, 8GB VRAM
- **Recommended**: 32GB RAM, 16-core CPU, 16GB VRAM
- **Optimal**: 160GB RAM, 48-core CPU, 16GB VRAM (current configuration)

---

## ğŸ“„ **License**

This project is part of a sequential storytelling pipeline for news article analysis.

---

## ğŸ™ **Acknowledgments**

- **HuggingFace** - Model hosting and transformers library
- **vLLM** - Optimized LLM inference
- **spaCy** - Dependency parsing
- **Sentence Transformers** - Event embeddings
- **FastAPI** - Web framework
- **Celery + Dask** - Distributed task processing

---

**Built with Claude Code v2.0.60 | Powered by Claude Sonnet 4.5**
